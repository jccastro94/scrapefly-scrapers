{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from urllib.parse import urlencode\n",
    "from scrapfly import ScrapflyClient, ScrapeConfig, ScrapeApiResponse\n",
    "from parsel import Selector\n",
    "import tracemalloc\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapfly = ScrapflyClient(key=\"scp-live-b77441dce98d408ea07b4bd966b558c9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def request_hotels_page(\n",
    "    query,\n",
    "    checkin: str = \"\",\n",
    "    checkout: str = \"\",\n",
    "    number_of_rooms=1,\n",
    "    offset: int = 0,\n",
    "):\n",
    "    \"\"\"scrapes a single hotel search page of booking.com\"\"\"\n",
    "    checkin_year, checking_month, checking_day = checkin.split(\"-\") if checkin else \"\", \"\", \"\"\n",
    "    checkout_year, checkout_month, checkout_day = checkout.split(\"-\") if checkout else \"\", \"\", \"\"\n",
    "\n",
    "    url = \"https://www.booking.com/searchresults.html\"\n",
    "    url += \"?\" + urlencode(\n",
    "        {\n",
    "            \"ss\": query,\n",
    "            \"checkin_year\": checkin_year,\n",
    "            \"checkin_month\": checking_month,\n",
    "            \"checkin_monthday\": checking_day,\n",
    "            \"checkout_year\": checkout_year,\n",
    "            \"checkout_month\": checkout_month,\n",
    "            \"checkout_monthday\": checkout_day,\n",
    "            \"no_rooms\": number_of_rooms,\n",
    "            \"offset\": offset,\n",
    "        }\n",
    "    )\n",
    "    return await scrapfly.async_scrape(ScrapeConfig(url, country=\"US\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_total_results(html):\n",
    "    sel = Selector(text=html)\n",
    "    h1_text = sel.css(\"h1::text\").get()\n",
    "    if h1_text:\n",
    "        match = re.search(r\"([\\d,]+)\\s+properties\\s+found\", h1_text)\n",
    "        if match:\n",
    "            return int(match.group(1).replace(\",\", \"\"))\n",
    "    # If no match found, check for alternative patterns or return a default value\n",
    "    return 0  # or some other appropriate default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_hotels(html: str):\n",
    "    sel = Selector(text=html)\n",
    "\n",
    "    hotel_previews = {}\n",
    "    for hotel_box in sel.xpath('//div[@data-testid=\"property-card\"]'):\n",
    "        url = hotel_box.xpath('.//h3/a[@data-testid=\"title-link\"]/@href').get(\"\").split(\"?\")[0]\n",
    "        hotel_previews[url] = {\n",
    "            \"name\": hotel_box.xpath('.//h3/a[@data-testid=\"title-link\"]/div/text()').get(\"\"),\n",
    "            \"location\": hotel_box.xpath('.//span[@data-testid=\"address\"]/text()').get(\"\"),\n",
    "            \"score\": hotel_box.xpath('.//div[@data-testid=\"review-score\"]/div/text()').get(\"\"),\n",
    "            \"review_count\": hotel_box.xpath('.//div[@data-testid=\"review-score\"]/div[2]/div[2]/text()').get(\"\"),\n",
    "            \"stars\": len(hotel_box.xpath('.//div[@data-testid=\"rating-stars\"]/span').getall()),\n",
    "            \"image\": hotel_box.xpath('.//img[@data-testid=\"image\"]/@src').get(),\n",
    "        }\n",
    "    return hotel_previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_search(\n",
    "    query,\n",
    "    checkin: str = \"\",\n",
    "    checkout: str = \"\",\n",
    "    number_of_rooms=1,\n",
    "    max_results: Optional[int] = None,\n",
    "):\n",
    "    first_page = await request_hotels_page(\n",
    "        query=query, checkin=checkin, checkout=checkout, number_of_rooms=number_of_rooms\n",
    "    )\n",
    "    hotel_previews = parse_search_hotels(first_page.content)\n",
    "    total_results = parse_search_total_results(first_page.content)\n",
    "    if max_results and total_results > max_results:\n",
    "        total_results = max_results\n",
    "    other_pages = await asyncio.gather(\n",
    "        *[\n",
    "            request_hotels_page(\n",
    "                query=query,\n",
    "                checkin=checkin,\n",
    "                checkout=checkout,\n",
    "                number_of_rooms=number_of_rooms,\n",
    "                offset=offset,\n",
    "            )\n",
    "            for offset in range(25, total_results, 25)\n",
    "        ]\n",
    "    )\n",
    "    for result in other_pages:\n",
    "        hotel_previews.update(parse_search_hotels(result.content))\n",
    "    return hotel_previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hotel(html: str):\n",
    "    sel = Selector(text=html)\n",
    "    css = lambda selector, sep=\"\": sep.join(sel.css(selector).getall()).strip()\n",
    "    css_first = lambda selector: sel.css(selector).get(\"\")\n",
    "    lat, lng = css_first(\".show_map_hp_link::attr(data-atlas-latlng)\").split(\",\")\n",
    "    features = defaultdict(list)\n",
    "    for feat_box in sel.css(\"[data-capla-component*=FacilitiesBlock]>div>div>div\"):\n",
    "        type_ = feat_box.xpath('.//span[contains(@data-testid, \"facility-group-icon\")]/../text()').get()\n",
    "        feats = [f.strip() for f in feat_box.css(\"li ::text\").getall() if f.strip()]\n",
    "        features[type_] = feats\n",
    "    data = {\n",
    "        \"title\": css(\"h2#hp_hotel_name::text\"),\n",
    "        \"description\": css(\"div#property_description_content ::text\", \"\\n\"),\n",
    "        \"address\": css(\".hp_address_subtitle::text\"),\n",
    "        \"lat\": lat,\n",
    "        \"lng\": lng,\n",
    "        \"features\": dict(features),\n",
    "        \"id\": re.findall(r\"b_hotel_id:\\s*'(.+?)'\", html)[0],\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_prices(hotel_id, csrf_token, hotel_url, start_date, duration, days_to_check):\n",
    "    prices = {}\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    for day in range(days_to_check):\n",
    "        check_date = start + timedelta(days=day)\n",
    "        data = {\n",
    "            \"name\": \"hotel.availability_calendar\",\n",
    "            \"result_format\": \"price_histogram\",\n",
    "            \"hotel_id\": hotel_id,\n",
    "            \"search_config\": json.dumps(\n",
    "                {\n",
    "                    \"b_adults_total\": 2,\n",
    "                    \"b_nr_rooms_needed\": 1,\n",
    "                    \"b_children_total\": 0,\n",
    "                    \"b_children_ages_total\": [],\n",
    "                    \"b_is_group_search\": 0,\n",
    "                    \"b_pets_total\": 0,\n",
    "                    \"b_rooms\": [{\"b_adults\": 2, \"b_room_order\": 1}],\n",
    "                }\n",
    "            ),\n",
    "            \"checkin\": check_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"n_days\": duration,\n",
    "            \"respect_min_los_restriction\": 1,\n",
    "            \"los\": duration,\n",
    "        }\n",
    "        result = await scrapfly.async_scrape(\n",
    "            ScrapeConfig(\n",
    "                url=\"https://www.booking.com/fragment.json?cur_currency=usd\",\n",
    "                method=\"POST\",\n",
    "                data=data,\n",
    "                headers={\"X-Booking-CSRF\": csrf_token},\n",
    "                session=hotel_url.split(\"/\")[-1].split(\".\")[0],\n",
    "                country=\"US\",\n",
    "            )\n",
    "        )\n",
    "        price_data = json.loads(result.content)[\"data\"]\n",
    "        prices[check_date.strftime(\"%Y-%m-%d\")] = price_data\n",
    "        \n",
    "        # Add a delay to avoid sending too many requests at once\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_hotel_availability(url: str, start_date: str, durations: List[int], days_to_check: int):\n",
    "    result = await scrapfly.async_scrape(ScrapeConfig(\n",
    "        url, \n",
    "        session=url.split(\"/\")[-1].split(\".\")[0],\n",
    "        country=\"US\",\n",
    "    ))\n",
    "    hotel = parse_hotel(result.content)\n",
    "    hotel[\"url\"] = result.context['url']\n",
    "    csrf_token = re.findall(r\"b_csrf_token:\\s*'(.+?)'\", result.content)[0]\n",
    "    \n",
    "    hotel[\"availability\"] = {}\n",
    "    for duration in durations:\n",
    "        hotel[\"availability\"][duration] = await scrape_prices(\n",
    "            csrf_token=csrf_token,\n",
    "            hotel_id=hotel[\"id\"],\n",
    "            hotel_url=url,\n",
    "            start_date=start_date,\n",
    "            duration=duration,\n",
    "            days_to_check=days_to_check\n",
    "        )\n",
    "    \n",
    "    return hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_hotel_scraper(hotel_urls: List[str], start_date: str, durations: List[int], days_to_check: int):\n",
    "    out = Path(os.getcwd()) / \"results\"\n",
    "    out.mkdir(exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    for url in hotel_urls:\n",
    "        hotel_data = await scrape_hotel_availability(url, start_date, durations, days_to_check)\n",
    "        results.append(hotel_data)\n",
    "        \n",
    "        # Add a delay between processing each hotel to avoid being too aggressive\n",
    "        await asyncio.sleep(5)\n",
    "\n",
    "    out.joinpath(\"hotel_availability.json\").write_text(json.dumps(results, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_hotels(urls: List[str], price_start_dt: str, price_n_days=30):\n",
    "    async def scrape_hotel(url: str):\n",
    "        result = await scrapfly.async_scrape(ScrapeConfig(\n",
    "            url, \n",
    "            session=url.split(\"/\")[-1].split(\".\")[0],\n",
    "            country=\"US\",\n",
    "        ))\n",
    "        hotel = parse_hotel(result.content)\n",
    "        hotel[\"url\"] = result.context['url']\n",
    "        csrf_token = re.findall(r\"b_csrf_token:\\s*'(.+?)'\", result.content)[0]\n",
    "        hotel[\"price\"] = await scrape_prices(csrf_token=csrf_token, hotel_id=hotel[\"id\"], hotel_url=url)\n",
    "        return hotel\n",
    "\n",
    "    async def scrape_prices(hotel_id, csrf_token, hotel_url):\n",
    "        data = {\n",
    "            \"name\": \"hotel.availability_calendar\",\n",
    "            \"result_format\": \"price_histogram\",\n",
    "            \"hotel_id\": hotel_id,\n",
    "            \"search_config\": json.dumps(\n",
    "                {\n",
    "                    # we can adjust pricing configuration here but this is the default\n",
    "                    \"b_adults_total\": 2,\n",
    "                    \"b_nr_rooms_needed\": 1,\n",
    "                    \"b_children_total\": 0,\n",
    "                    \"b_children_ages_total\": [],\n",
    "                    \"b_is_group_search\": 0,\n",
    "                    \"b_pets_total\": 0,\n",
    "                    \"b_rooms\": [{\"b_adults\": 2, \"b_room_order\": 1}],\n",
    "                }\n",
    "            ),\n",
    "            \"checkin\": price_start_dt,\n",
    "            \"n_days\": price_n_days,\n",
    "            \"respect_min_los_restriction\": 1,\n",
    "            \"los\": 1,\n",
    "        }\n",
    "        result = await scrapfly.async_scrape(\n",
    "            ScrapeConfig(\n",
    "                url=\"https://www.booking.com/fragment.json?cur_currency=usd\",\n",
    "                method=\"POST\",\n",
    "                data=data,\n",
    "                headers={\"X-Booking-CSRF\": csrf_token},\n",
    "                session=hotel_url.split(\"/\")[-1].split(\".\")[0],\n",
    "                country=\"US\",\n",
    "            )\n",
    "        )\n",
    "        return json.loads(result.content)[\"data\"]\n",
    "\n",
    "    hotels = await asyncio.gather(*[scrape_hotel(url) for url in urls])\n",
    "    return hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_urls = [\n",
    "    \"https://www.booking.com/hotel/mx/century-zona-rosa.es.html\",\n",
    "    \"https://www.booking.com/hotel/mx/bristol.es.html\",\n",
    "    # Add more hotel URLs as needed\n",
    "]\n",
    "\n",
    "start_date = \"2024-08-8\" \n",
    "durations = [1, 3, 7] \n",
    "days_to_check = 90 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:<-- 200 | ERR::SCRAPE::BAD_UPSTREAM_RESPONSE - The website you target respond with an unexpected status code (>400) - The scrapped url: https://www.booking.com/fragment.json?cur_currency=usd respond with 400 - Bad Request: . Checkout the related doc: https://scrapfly.io/docs/scrape-api/error/ERR::SCRAPE::BAD_UPSTREAM_RESPONSE\n"
     ]
    },
    {
     "ename": "UpstreamHttpClientError",
     "evalue": "Target website responded with 400 - Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUpstreamHttpClientError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_hotel_scraper(hotel_urls, start_date, durations, days_to_check)\n",
      "Cell \u001b[1;32mIn[65], line 7\u001b[0m, in \u001b[0;36mrun_hotel_scraper\u001b[1;34m(hotel_urls, start_date, durations, days_to_check)\u001b[0m\n\u001b[0;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m hotel_urls:\n\u001b[1;32m----> 7\u001b[0m     hotel_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_hotel_availability(url, start_date, durations, days_to_check)\n\u001b[0;32m      8\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(hotel_data)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Add a delay between processing each hotel to avoid being too aggressive\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[64], line 13\u001b[0m, in \u001b[0;36mscrape_hotel_availability\u001b[1;34m(url, start_date, durations, days_to_check)\u001b[0m\n\u001b[0;32m     11\u001b[0m hotel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailability\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m duration \u001b[38;5;129;01min\u001b[39;00m durations:\n\u001b[1;32m---> 13\u001b[0m     hotel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailability\u001b[39m\u001b[38;5;124m\"\u001b[39m][duration] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_prices(\n\u001b[0;32m     14\u001b[0m         csrf_token\u001b[38;5;241m=\u001b[39mcsrf_token,\n\u001b[0;32m     15\u001b[0m         hotel_id\u001b[38;5;241m=\u001b[39mhotel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m         hotel_url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m     17\u001b[0m         start_date\u001b[38;5;241m=\u001b[39mstart_date,\n\u001b[0;32m     18\u001b[0m         duration\u001b[38;5;241m=\u001b[39mduration,\n\u001b[0;32m     19\u001b[0m         days_to_check\u001b[38;5;241m=\u001b[39mdays_to_check\n\u001b[0;32m     20\u001b[0m     )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hotel\n",
      "Cell \u001b[1;32mIn[63], line 27\u001b[0m, in \u001b[0;36mscrape_prices\u001b[1;34m(hotel_id, csrf_token, hotel_url, start_date, duration, days_to_check)\u001b[0m\n\u001b[0;32m      6\u001b[0m check_date \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mday)\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotel.availability_calendar\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_histogram\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlos\u001b[39m\u001b[38;5;124m\"\u001b[39m: duration,\n\u001b[0;32m     26\u001b[0m }\n\u001b[1;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrapfly\u001b[38;5;241m.\u001b[39masync_scrape(\n\u001b[0;32m     28\u001b[0m     ScrapeConfig(\n\u001b[0;32m     29\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.booking.com/fragment.json?cur_currency=usd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     32\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Booking-CSRF\u001b[39m\u001b[38;5;124m\"\u001b[39m: csrf_token},\n\u001b[0;32m     33\u001b[0m         session\u001b[38;5;241m=\u001b[39mhotel_url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     34\u001b[0m         country\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m price_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result\u001b[38;5;241m.\u001b[39mcontent)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     38\u001b[0m prices[check_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m=\u001b[39m price_data\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\scrapfly\\client.py:326\u001b[0m, in \u001b[0;36mScrapflyClient.async_scrape\u001b[1;34m(self, scrape_config, loop)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_executor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape, scrape_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\backoff\\_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[0;32m    102\u001b[0m }\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\scrapfly\\client.py:419\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[1;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_raise \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ScrapflyError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\scrapfly\\client.py:408\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[1;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[0;32m    406\u001b[0m request_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scrape_request(scrape_config\u001b[38;5;241m=\u001b[39mscrape_config)\n\u001b[0;32m    407\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_data)\n\u001b[1;32m--> 408\u001b[0m scrape_api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreporter\u001b[38;5;241m.\u001b[39mreport(scrape_api_response\u001b[38;5;241m=\u001b[39mscrape_api_response)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scrape_api_response\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\scrapfly\\client.py:423\u001b[0m, in \u001b[0;36mScrapflyClient._handle_response\u001b[1;34m(self, response, scrape_config)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response:Response, scrape_config:ScrapeConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScrapeApiResponse:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m         api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_api_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scrape_config\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    430\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<-- [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    431\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    432\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason,\n\u001b[0;32m    433\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    434\u001b[0m                 \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    435\u001b[0m             ))\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\scrapfly\\client.py:605\u001b[0m, in \u001b[0;36mScrapflyClient._handle_api_response\u001b[1;34m(self, response, scrape_config, raise_on_upstream_error)\u001b[0m\n\u001b[0;32m    596\u001b[0m         body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    598\u001b[0m api_response:ScrapeApiResponse \u001b[38;5;241m=\u001b[39m ScrapeApiResponse(\n\u001b[0;32m    599\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m    600\u001b[0m     request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m    601\u001b[0m     api_result\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    602\u001b[0m     scrape_config\u001b[38;5;241m=\u001b[39mscrape_config\n\u001b[0;32m    603\u001b[0m )\n\u001b[1;32m--> 605\u001b[0m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api_response\n",
      "File \u001b[1;32mc:\\Users\\jccas\\Documents\\Code\\Personal\\scrapers\\.venv\\Lib\\site-packages\\scrapfly\\api_response.py:433\u001b[0m, in \u001b[0;36mScrapeApiResponse.raise_for_result\u001b[1;34m(self, raise_on_upstream_error)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, UpstreamHttpError):\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_upstream_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mUpstreamHttpClientError\u001b[0m: Target website responded with 400 - Bad Request"
     ]
    }
   ],
   "source": [
    "await run_hotel_scraper(hotel_urls, start_date, durations, days_to_check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
